{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from operator import itemgetter\n",
    "from pprint import pprint\n",
    "#import unidecode\n",
    "import requests\n",
    "\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from geopy import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Create scraping functions\n",
    "\n",
    "### The below codes are to make 2 dataframes for our Data Analytics submission\n",
    "#### The first function scrape restaurant features such as user rating, number of ratings, address and a few keywords specified on netpincer\n",
    "#### The second function collects information a restaurant's products and their corresponding prices as advertised on the site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_base = 'https://www.netpincer.hu/'\n",
    "\n",
    "\n",
    "#create empty lists+dataframes to enrich with user defined functions\n",
    "link_list=[]\n",
    "product_df = pd.DataFrame(columns = ['Restaurant', 'Product_Name', 'Price']) \n",
    "restaurant_df = pd.DataFrame(columns = ['Restaurant', 'User-Rating',  'No-Ratings', 'Address', 'Feature1'\n",
    "                                       , 'Feature2','Feature3', 'Feature4', 'Feature5']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse netpincer to get list of html links for restaurants available at BASE_URL\n",
    "\n",
    "def generate_restaurant_links(URL):\n",
    "    global link_base\n",
    "    global link_list\n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\") \n",
    "    restaurants = soup.find('ul', {'class':'vendor-list'}).findAll('a')\n",
    "    link_list.extend([link_base+link.get('href') for link in restaurants])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restaurant_scraper(URL):\n",
    "    \n",
    "    global restaurant_df\n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\") \n",
    "\n",
    "    #Getting name of the restaurant\n",
    "    restaurant = soup.find('div',{'class':'vendor-info-main-headline item'}).getText().replace('\\n','')\n",
    "\n",
    "    #Getting features of restaurant\n",
    "    features = soup.find('ul',{'class':'vendor-info-main-details-cuisines'}).findAll('li')\n",
    "    features_list = [values.getText() for values in features[1:]]\n",
    "    #Adding nan values if feature is missing - the max amount of restaurant features are 5\n",
    "    features_list.extend((5- len(features_list)) * [np.nan])\n",
    "\n",
    "    #Getting ratings of restaurants if available - error handling was needed and to replace values with NaNs \n",
    "    try:\n",
    "        ratings = soup.find('div',{'class':'ratings-component'}).findAll('span')\n",
    "        ratings_list = [values.getText().split()for values in ratings[1:]]\n",
    "            #Splitting out user rating out of how many\n",
    "        ratings_list = [nums[0].split('/') for nums in ratings_list]\n",
    "            #Breaking out lists in list\n",
    "        ratings_list = [[item] for sub_list in ratings_list for item in sub_list]\n",
    "    except:\n",
    "        ratings_list = [[np.NaN],[np.NaN],[np.NaN]]\n",
    "\n",
    "    #Getting the address of a Restaurant\n",
    "    address = soup.find('p',{'class':'vendor-location'}).getText()\n",
    "\n",
    "    #Putting all of the above in a dictionary\n",
    "    dict_for_restaurant_df = {'Restaurant': restaurant, 'User-Rating':ratings_list[0][0],  'No-Ratings': ratings_list[2][0]\n",
    "                              , 'Address' : address, 'Feature1': features_list[0], 'Feature2': features_list[1]\n",
    "                              ,'Feature3': features_list[2], 'Feature4' : features_list[3],'Feature5': features_list[4]}\n",
    "\n",
    "    #Add to restaurant_df\n",
    "    restaurant_df = restaurant_df.append(dict_for_restaurant_df, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_scraper (URL): \n",
    "    global product_df\n",
    "    prices = []\n",
    "    \n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\") \n",
    "\n",
    "    #Getting name of the restaurant\n",
    "    restaurant = soup.find('div',{'class':'vendor-info-main-headline item'}).getText().replace('\\n','')\n",
    "    \n",
    "    #Getting name of product and prices - fortunately both are spans\n",
    "    product_names = soup.find('div', {'class':'menu__items'}).findAll('h3',{'class':'dish-name fn p-name'})\n",
    "    product_names = [name.getText().replace('\\n','') for name in product_names]\n",
    "    \n",
    "    #Getting price of a product - certain formattings are required as many unused characters were stored in spans\n",
    "    prices_temp = soup.find('div', {'class':'menu__items'}).findAll('span',{'class':'price p-price'})\n",
    "    \n",
    "    for price in prices_temp:\n",
    "        price = price.getText()\n",
    "        price = price.replace(\"\\n\",\"\").replace(\" \",\"\").replace(\"innen\",\"\").replace(\"\\xa0\",\"\").split('Ft', 1)[0]\n",
    "        prices.append(price)\n",
    "   \n",
    "    #Creating restaurant list which should be as long as other lists that we created for products\n",
    "    restaurant_name = [restaurant for i in range(len(product_names))]\n",
    "\n",
    "    #Put vectors into pandas dataframe    \n",
    "    dict_for_df = {'Restaurant': restaurant_name, 'Product_Name': product_names, 'Price': prices}  \n",
    "    df = pd.DataFrame(dict_for_df)\n",
    "    \n",
    "    product_df = product_df.append(df, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Scrape restaurants\n",
    "\n",
    "### We scrape data and structre them in tidy data tables\n",
    "####  We first decide on the population of resturants to include - and their netpincer URLs:\n",
    "##### - Pizzaplaces that deliver to CEU\n",
    "##### - Pizzaplaces that can deliver to the city centers of the top five Hungarian cities (excluding Budapest): Debrecen, Szeged, Miskolc, Pecs, Gyor\n",
    "#### Then we run both restaurant and product scrapers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1) Let's start with the restaurant scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_restaurant_df = pd.DataFrame(columns = ['Restaurant', 'User-Rating',  'No-Ratings', 'Address', 'Feature1'\n",
    "                                   , 'Feature2','Feature3', 'Feature4', 'Feature5','City']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BP_URL = 'https://www.netpincer.hu/restaurants/new?lat=47.501185&lng=19.049364&vertical=restaurants&cuisines=52'\n",
    "DB_URL = 'https://www.netpincer.hu/restaurants/new?lat=47.5313352&lng=21.624532&vertical=restaurants&cuisines=52'\n",
    "SZG_URL = 'https://www.netpincer.hu/restaurants/new?lat=46.254233&lng=20.1493499&vertical=restaurants&cuisines=52'\n",
    "MS_URL = 'https://www.netpincer.hu/restaurants/new?lat=48.10137599999999&lng=20.7306244&vertical=restaurants&cuisines=52'\n",
    "PCS_URL = 'https://www.netpincer.hu/restaurants/new?lat=46.07605239999999&lng=18.2282426&vertical=restaurants&cuisines=52'\n",
    "\n",
    "cities = {'Budapest':BP_URL,'Debrecen':DB_URL,'Szeged':SZG_URL,'Miskolc': MS_URL,'PÃ©cs': PCS_URL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in cities:\n",
    "    link_list = []\n",
    "    restaurant_df = pd.DataFrame(columns = ['Restaurant', 'User-Rating',  'No-Ratings', 'Address', 'Feature1'\n",
    "                                           , 'Feature2','Feature3', 'Feature4', 'Feature5']) \n",
    "\n",
    "    generate_restaurant_links(cities[city])\n",
    "    for link in link_list:\n",
    "        restaurant_scraper(link)\n",
    "        print(link+\" is ready\")\n",
    "\n",
    "    city_list = len(restaurant_df)*[city]\n",
    "    restaurant_df[\"City\"] = np.array(city_list)\n",
    "\n",
    "    final_restaurant_df = final_restaurant_df.append(restaurant_df,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_restaurant_df.to_csv(r'C:\\Users\\T450s\\Python_directory\\all_restaurants.csv', \n",
    "#                           index = True, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_restaurant_df = pd.read_csv(r'C:\\Users\\T450s\\Python_directory\\all_restaurants.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating address + city for geolocator input\n",
    "dist_restaurants_df = final_restaurant_df\n",
    "dist_restaurants_df['ProperAddress'] = dist_restaurants_df['City'] + ' ' + dist_restaurants_df['Address']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting city center locations #TODO: store in list\n",
    "locator = Nominatim(user_agent='ba')\n",
    "location_bud = locator.geocode('Budapest')\n",
    "location_deb = locator.geocode('Debrecen')\n",
    "location_szg = locator.geocode('Szeged')\n",
    "location_ms = locator.geocode('Miskolc')\n",
    "location_pcs = locator.geocode('PÃ©cs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RateLimiter must be used for Nominatim\n",
    "geocoder = RateLimiter(Nominatim(user_agent='ba').geocode, min_delay_seconds=1)\n",
    "dist_restaurants_df['Location'] = (dist_restaurants_df['ProperAddress']).apply(geocoder)\n",
    "\n",
    "# add restaurant latitude and longitude to dataframe\n",
    "dist_restaurants_df['RestaurantLatitude'] = dist_restaurants_df['Location'].apply(lambda loc: loc.latitude if loc else None)\n",
    "dist_restaurants_df['RestaurantLongitude'] = dist_restaurants_df['Location'].apply(lambda loc: loc.longitude if loc else None)\n",
    "\n",
    "#add city center latitude and logitude to dataframe\n",
    "dist_restaurants_df.loc[dist_restaurants_df['City'] == 'Budapest', 'CenterLatitude'] = location_bud.latitude\n",
    "dist_restaurants_df.loc[dist_restaurants_df['City'] == 'Budapest', 'CenterLongitude'] = location_bud.longitude\n",
    "dist_restaurants_df.loc[dist_restaurants_df['City'] == 'Debrecen', 'CenterLatitude'] = location_deb.latitude\n",
    "dist_restaurants_df.loc[dist_restaurants_df['City'] == 'Debrecen', 'CenterLongitude'] = location_deb.longitude\n",
    "dist_restaurants_df.loc[dist_restaurants_df['City'] == 'Szeged', 'CenterLatitude'] = location_szg.latitude\n",
    "dist_restaurants_df.loc[dist_restaurants_df['City'] == 'Szeged', 'CenterLongitude'] = location_szg.longitude\n",
    "dist_restaurants_df.loc[dist_restaurants_df['City'] == 'Miskolc', 'CenterLatitude'] = location_ms.latitude\n",
    "dist_restaurants_df.loc[dist_restaurants_df['City'] == 'Miskolc', 'CenterLongitude'] = location_ms.longitude\n",
    "dist_restaurants_df.loc[dist_restaurants_df['City'] == 'PÃ©cs', 'CenterLatitude'] = location_pcs.latitude\n",
    "dist_restaurants_df.loc[dist_restaurants_df['City'] == 'PÃ©cs', 'CenterLongitude'] = location_pcs.longitude\n",
    "#TODO use loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate distance\n",
    "#drop rows with no proper location\n",
    "dist_restaurants_df = dist_restaurants_df[dist_restaurants_df['RestaurantLatitude'].notna()]\n",
    "dist_restaurants_df = dist_restaurants_df[dist_restaurants_df['CenterLatitude'].notna()]\n",
    "dist_restaurants_df['distance'] = dist_restaurants_df.apply(lambda row: distance.distance((row.RestaurantLatitude, row.RestaurantLongitude), (row.CenterLatitude, row.CenterLongitude)), axis=1 if distance.distance else None)\n",
    "dist_restaurants_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dist_restaurants_df.to_csv(r'/Users/utassydv/Downloads/all_restaurants_w_dist.csv', \n",
    "#                           index = True, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) And now the product scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = pd.DataFrame(columns = ['Restaurant', 'Product_Name', 'Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in cities:\n",
    "    link_list = []\n",
    "    \n",
    "    generate_restaurant_links(cities[city])\n",
    "    for link in link_list:\n",
    "        product_scraper(link)\n",
    "        print(link+\" is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#product_df.to_csv(r'C:\\Users\\T450s\\Python_directory\\all_products.csv', \n",
    "#                           index = True, sep=',', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
